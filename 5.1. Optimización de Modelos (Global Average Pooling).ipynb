{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Optimización de Modelos (Global Average Pooling)\n",
    "\n",
    "En el notebook anterior decidimos optimizar los siguientes modelos:\n",
    "\n",
    " - Rank=1, Name=**SCV-linear-1.0**, Score=**0.9934480277920665 (+/- 0.003944167917138149)**\n",
    " - Rank=9, Name=**SGDClassifier**, Score=**0.9916728170032577 (+/- 0.004665749258440611)**\n",
    " - Rank=11, Name=**RidgeClassifier-0.8**, Score=**0.9912639128068644 (+/- 0.004574692652304791)**\n",
    " - Rank=24, Name=**KNeighborsClassifier-4**, Score=**0.9892162234696844 (+/- 0.00614647322899209)**\n",
    " \n",
    "Para optimizar cada modelo, usaremos GridSearchCV, el cual es un método para automáticamente encontrar los mejores parámetros de un modelo, con base en una métrica objetivo que, en nuestro caso, es la exactitud (_accuracy_).\n",
    "\n",
    "Las iniciales CV significan que usaremos _cross-validation_ o validación cruzada para estar verdaderamente seguros del desempeño del modelo. Como sudió en la fase de Evaluación de Modelos, haremos 10 validaciones cruzadas por modelo.\n",
    "\n",
    "Como podemos esperar, este proceso tomará algo de tiempo, dado que no sólo entrenaremos muchos modelos, sino que cada variación necesitará de otros 10 modelos, debido al proceso de validación cruzada.\n",
    "\n",
    "Empecemos definiendo las funciones auxiliares que usaremos para optimizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos\n",
    "\n",
    "Esta función nos retornará los datos relevantes necesatios para llevar a cabo la optimización de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset():\n",
    "    X = np.load('global_average_features.npy')\n",
    "    y = np.load('labels.npy')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos\n",
    "\n",
    "Esta función devuelve un dict de dicts que contiene el modelo a ser optimizado, junto con la grilla de parámetros para hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def define_models(models=dict()):\n",
    "    models['SGDClassifier'] = {\n",
    "        'model': SGDClassifier(),\n",
    "        'parameters': {\n",
    "            'loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'),\n",
    "            'penalty': ('none','l2', 'l1'),\n",
    "            'alpha': (1e-3, 5e-3, 1e-4, 1e-2, 5e-3),\n",
    "            'fit_intercept': (True, False)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    models['KNeighborsClassifier-4'] = {\n",
    "        'model': KNeighborsClassifier(n_neighbors=4),\n",
    "        'parameters': {\n",
    "            'weights': ('uniform', 'distance'),\n",
    "            'algorithm': ('auto', 'ball_tree', 'kd_tree', 'brute')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    models['SVC-linear-1.0'] = {\n",
    "        'model': SVC(kernel='linear', C=1.0),\n",
    "        'parameters': {\n",
    "            'probability': (True, False),\n",
    "            'decision_function_shape': ('ovo', 'ovr'),\n",
    "            'shrinking': (True, False)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    models['RidgeClassifier-0.8'] = {\n",
    "        'model': RidgeClassifier(alpha=0.8),\n",
    "        'parameters': {\n",
    "            'fit_intercept': (True, False),\n",
    "            'alpha': (1e-3, 5e-3, 1e-4, 1e-2, 5e-3),\n",
    "            'solver': ('svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f'Defined {len(models)} models.')\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Como en el notebook pasado, este pipeline es usado para pre-procesar los _features_ que le pasaremos al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def make_pipeline(model):\n",
    "    steps = [\n",
    "        ('StandardScaler', StandardScaler()),\n",
    "        ('MinMaxScaler', MinMaxScaler()),\n",
    "        ('model', model)\n",
    "    ]\n",
    "    \n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de Paráemtros\n",
    "\n",
    "Aquí definimos un par de funciones auxiliares que llevarán a cabo la búsqueda de parámetros que optimizarán cada modelo. Al final del proceso, se imprimirán los mejores parámetros, junto con el puntaje/exactitud del mejor clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_model(X, y, model, folds, metric):\n",
    "    m = model['model'] \n",
    "    parameters = model['parameters']\n",
    "    \n",
    "    classifier = GridSearchCV(m, parameters, cv=folds, scoring=metric, n_jobs=-1)\n",
    "    pipeline = make_pipeline(classifier)\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    return pipeline.steps[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def grid_search_models(X, y, models, folds=10, metric='accuracy'):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        for model_name, model in models.items():\n",
    "            m = grid_search_model(X, y, model, folds, metric)\n",
    "\n",
    "            if m is not None:\n",
    "                print(f'Best parameters for {model_name}: \\n{m.best_params_}')\n",
    "                print(f'Best model {metric}: {m.best_score_ * 100}%')\n",
    "            else:\n",
    "                print(f'{model_name}: error')\n",
    "            \n",
    "            print('----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 4 models.\n",
      "Best parameters for SGDClassifier: \n",
      "{'alpha': 0.0001, 'fit_intercept': False, 'loss': 'log', 'penalty': 'none'}\n",
      "Best model accuracy: 99.2901023890785%\n",
      "----\n",
      "\n",
      "Best parameters for KNeighborsClassifier-4: \n",
      "{'algorithm': 'brute', 'weights': 'distance'}\n",
      "Best model accuracy: 99.0853242320819%\n",
      "----\n",
      "\n",
      "Best parameters for SVC-linear-1.0: \n",
      "{'decision_function_shape': 'ovo', 'probability': True, 'shrinking': True}\n",
      "Best model accuracy: 99.35836177474403%\n",
      "----\n",
      "\n",
      "Best parameters for RidgeClassifier-0.8: \n",
      "{'alpha': 0.001, 'fit_intercept': True, 'solver': 'lsqr'}\n",
      "Best model accuracy: 99.0443686006826%\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset()\n",
    "models = define_models()\n",
    "grid_search_models(X, y, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Estos son nuestros resultados:\n",
    "\n",
    " - **SVC-linear-1.0** pasa de 99.34480277920665% a 99.35836177474403%, con una mejora total de 0.01355899553738027%.\n",
    " - **RidgeClassifier-0.8** pasa de 99.12639128068644% a 99.0443686006826%, con una desmejora total de 0.08202268000383128%.\n",
    " - **SGDClassifier** pasa de 99.16728170032577% a 99.2901023890785%, con una mejora total de 0.122820688752725%.\n",
    " - **KNeighborsClassifier-4** pasa de 98.92162234696844% a 99.0853242320819%, con una mejora total de 0.1637018851134684%.\n",
    " \n",
    "Observamos que a excepción de RidgeClassifier-0.8, todos los modelos muestran, al menos, una pequeña mejora en su exactitud, lo que significa que la búsqueda de parámetros fue exitosa.\n",
    "\n",
    "Claramente, el modelo que más se benefició fue KNeighborsClassifier-4, con una mejora total de 0.1637018851134684%. \n",
    "\n",
    "El mejor puntaje lo alcanzó SVC-linear-1.0: 99.35836177474403%.\n",
    "\n",
    "Lamentablemente, RidgeClassifier-0.8 se desempeña peor al final del procedimiento. Quizás jugar con otros parámetros de mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "\n",
    " - Probar otros modelos.\n",
    " - Probar más parámetros.\n",
    " - Probar reducción de dimensiones.\n",
    " - Usar más técnicas de ingeniería de _features_.\n",
    " - Usar _stacking_.\n",
    " \n",
    "### Una nota final\n",
    "\n",
    "No usamos la versión aplanada de los datos porque no cupo en la memoria de mi computador :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
