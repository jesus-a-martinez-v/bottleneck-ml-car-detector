{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bottleneck Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest advantages of transfer learning is that it let us harness the vast amounts of knowledge stored in the layers of the pre-trained models at our disposal. In a world where data is costly in time and money to acquire, this characteristic is quite appealing.\n",
    "\n",
    "If we think about it, these pre-trained models can act as excelent feature extractors, if we decide to drop the top of them (which tends to be a fully-connected network used for classification on ImageNet). \n",
    "\n",
    "With these features in hand, we can append _any_ machine learning model, including (but not limited to) neural networks, as long as we perform the proper the proper transformations required by the receiving algorithm.\n",
    "\n",
    "The features that result from this approach are known as __bottleneck features__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractors\n",
    "\n",
    "Although any pre-trained model can act as a feature extractor, in [this]() project we determined that the pre-trained model that works best for this problem is VGG16, so it is only logical to reuse it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "base = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define three feature extractors:\n",
    "\n",
    "  - **Global Average Pooling Extractor**: For this extractor we are applying a GlobalAveragePooling2D operation to the outputs of the pre-trained model (without the top). A global average pooling calculates, as it name indicates, the average of all the values in an activation map (also known as feature map, kernel or filter map). So, if we have a feature volume of 32 filters of 64x64, we'll get a vector of 32 elements.\n",
    "  - **Global Max Pooling Extractor**: For this extractor we are applying a GlobalMaxPooling2D operation to the outputs of the pre-trained model (without the top). A global max pooling calculates, as it name indicates, the maximum of all the values in an activation map (also known as feature map, kernel or filter map). So, if we have a feature volume of 32 filters of 64x64, we'll get a vector of 32 elements.\n",
    "  - **Flatten Extractor**: This is the simplest extractor, as it only reshapes the outputs of the model (without the top) into a very long vector. So, for instance, if we have a feature volume of 32 filters of 64x64, it will produce a vector of 64 * 32 * 32 = 65536 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPool2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x = base.output\n",
    "out = GlobalAveragePooling2D()(x)\n",
    "        \n",
    "global_average_feature_extractor = Model(inputs=base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base.output\n",
    "out = GlobalMaxPool2D()(x)\n",
    "        \n",
    "global_max_feature_extractor = Model(inputs=base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base.output\n",
    "out = Flatten()(x)\n",
    "        \n",
    "flatten_feature_extractor = Model(inputs=base.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Transforms\n",
    "\n",
    "With these extractors in place, we can use them to generate a transform of the original dataset. Our goal with this project is to use deep learning to generate good features for more classical machine learning algorithms, such as decision trees, or SVMs.\n",
    "\n",
    "The original images will be passed through each feature extractor, and the features and labels will be saved in a `.npy` file, which is a format that NumPy likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "\n",
    "These two functions will let us load an image, convert it into a tensor and pre-process it the way VGG16 expects them to be pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def path_to_tensor(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    \n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(image_paths):\n",
    "    tensors = [path_to_tensor(image_path) for image_path in image_paths]\n",
    "    return np.vstack(tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before transforming all the data, let's make sure everything is working with just a couple of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 3\n",
    "\n",
    "image_paths = glob('dataset/train/*/*')\n",
    "image_paths = random.sample(image_paths, SAMPLE_SIZE)\n",
    "\n",
    "# Calculate the image input\n",
    "image_input = preprocess_input(paths_to_tensor(image_paths))\n",
    "\n",
    "print(image_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shape above correspond to the batch we'll pass to each feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottlenecked batch shape (Global Max): (3, 512)\n",
      "Bottlenecked batch shape (Global Average): (3, 512)\n",
      "Bottlenecked batch shape (Flatten): (3, 25088)\n"
     ]
    }
   ],
   "source": [
    "global_max_bottlenecked_batch = global_max_feature_extractor.predict(image_input)\n",
    "print(f'Bottlenecked batch shape (Global Max): {global_max_bottlenecked_batch.shape}')\n",
    "\n",
    "global_average_bottlenecked_batch = global_average_feature_extractor.predict(image_input)\n",
    "print(f'Bottlenecked batch shape (Global Average): {global_average_bottlenecked_batch.shape}')\n",
    "\n",
    "flatten_bottlenecked_batch = flatten_feature_extractor.predict(image_input)\n",
    "print(f'Bottlenecked batch shape (Flatten): {flatten_bottlenecked_batch.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the Flatten Extractor, as expected, generates the longest representation of the images, as it does not summarize anything, unlike the other two extractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Three Versions of the Data\n",
    "\n",
    "We are now good to go! Let's generate each transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (7325, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if not Path('features.npy').is_file():\n",
    "    vehicles_images_path = glob('data/vehicles/*/*.png')\n",
    "    non_vehicles_images_path = glob('data/non-vehicles/*/*.png')\n",
    "\n",
    "    vehicles_images = preprocess_input(paths_to_tensor(vehicles_images_path))\n",
    "    non_vehicles_images = preprocess_input(paths_to_tensor(non_vehicles_images_path))\n",
    "\n",
    "    features = np.vstack([vehicles_images, non_vehicles_images])\n",
    "\n",
    "    np.save('features.npy', features)\n",
    "else:\n",
    "    features = np.load('features.npy')\n",
    "    \n",
    "print(f'Features shape: {features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (7325,)\n"
     ]
    }
   ],
   "source": [
    "if not Path('labels.npy').is_file():\n",
    "    vehicles_labels = np.array([1] * len(vehicles_images_path))\n",
    "    non_vehicles_labels = np.array([0] * len(non_vehicles_images_path))\n",
    "    labels = np.hstack([vehicles_labels, non_vehicles_labels])\n",
    "    \n",
    "    np.save('labels.npy', labels)\n",
    "else:\n",
    "    labels = np.load('labels.npy')\n",
    "\n",
    "    print(f'Labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_average_features shape: (7325, 512)\n"
     ]
    }
   ],
   "source": [
    "if not Path('global_average_features.npy').is_file():\n",
    "    global_average_bottlenecked_features = global_average_feature_extractor.predict(features)\n",
    "    \n",
    "    np.save('global_average_features.npy', global_average_bottlenecked_features)\n",
    "else:\n",
    "    global_average_bottlenecked_features = np.load('global_average_features.npy')\n",
    "\n",
    "print(f'global_average_features shape: {global_average_bottlenecked_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_max_bottlenecked_features shape: (7325, 512)\n"
     ]
    }
   ],
   "source": [
    "if not Path('global_max_features.npy').is_file():\n",
    "    global_max_bottlenecked_features = global_max_feature_extractor.predict(features)\n",
    "    np.save('global_max_features.npy', global_max_bottlenecked_features)\n",
    "else:\n",
    "    global_max_bottlenecked_features = np.load('global_max_features.npy')\n",
    "\n",
    "print(f'global_max_bottlenecked_features shape: {global_max_bottlenecked_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_bottlenecked_features shape: (7325, 25088)\n"
     ]
    }
   ],
   "source": [
    "if not Path('flattened_features.npy').is_file():\n",
    "    flattened_bottlenecked_features = flatten_feature_extractor.predict(features)\n",
    "    np.save('flattened_features.npy', flattened_bottlenecked_features)\n",
    "else:\n",
    "    flattened_bottlenecked_features = np.load('flattened_features.npy')\n",
    "\n",
    "print(f'flattened_bottlenecked_features shape: {flattened_bottlenecked_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these transforms we can now proceed to evaluate different models using each of them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
